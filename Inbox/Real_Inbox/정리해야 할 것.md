

# Pytorch
- Pytorch의 핵심 : numpy + AutoGrad + Function
- tensor.device : 현재 메모리가 올려져 있는 장치 확인
- tensor.to('cuda') : GPU에 올림
- squeeze, unsqueeze : 차원의 개수가 1인 차원을 삭제 또는 추가
- cat, stack : cat은 있는 차원에 붙이는 것이고 stack은 없는 차원을 만들어서 붙이는 것
- 기본적인 operation은 ndarray와 동일
- nn.functional에서 softmax, one_hot, cartesian product(모든 경우의 수 계산) 등 지원
- tensor.type() - 텐서의 타입 반환 및 dtype(데이터 타입) 변경
- tensor.dtype - 텐서의 데이터 타입 반환
- torch.numel(tensor)


# AutoGrad
- requires_grad를 통해 업데이트 가능한지 설정.
- 목적함수를 정의한 후 backward()를 통해 미분.
- w.grad 하면 해당 가중치의 미분값이 출력됨.

# torch.nn.Module
- torch.nn.Module : layer를 만들기 위한 base class
- input, output, forward, backward 정의
- 학습의 대상이 되는 parameter(tensor) 정의
- nn.Parameter를 직접 사용할 일은 없다. 이미 Linear, Conv 등으로 기본적인 것들이 정의가 되어있기 때문.
	- nn.Parameter는 Tensor의 상속 객체
	- nn.Module 내에 attribute가 될 때는 required_grad=True를 통해 학습대상이 되는 Tensor
	- 하지만 tensor와는 엄연히 다른 것. tensor로 구현할 시 layer.parameter하면 값이 안나옴.
	  parameter로 구현할 시 layer.parameter하면 가중치 값이 출력됨.
- backward는 forward의 결과값과 정답값으로 구한 loss(스칼라)에 대한 미분을 수행. 이후 parameter 업데이트
- optimizer.zero_grad() : 이전 grad 값 삭제
- outputs = model(inputs), loss = criterion(outputs, labels)
- loss.backward(), optimizer.step()
- Module에서 backward와 optimizer 오버라이딩 가능, 하지만 실제로 사용자가 직접 미분 수식을 써야할 필요는 없음. AutoGrad가 있기 때문.

# Dataset & Dataloader
- Datatset은 `__init__(), __len__(), __getitem__()`가 정의 되어 있어야 한다.
- Dataloader는 데이터를 변형하고 섞고 batch를 만들어 반환하는 클래스이다.
- 데이터 입력 방식의 표준화를 이루기 위해 사용한다?
- Dataset 정의를 통해 task에 따라 정답값의 type이 바뀔 수 있다.
- Dataloader에서 sampler는 데이터 샘플링을 어떻게 할지 설정하고, collate_fn은 반환되는 데이터의 형태를 변환하거나 가변적인 길이의 데이터일때 사용할 수 있다.

# Python
깊은 복사, 얕은 복사 정리
isinstance ex) isinstance(a, torch.Tensor)

namedtuple(https://wikidocs.net/126315), ordereddict(https://www.daleseo.com/python-collections-ordered-dict/), getitem, getattr(https://jeonghyeokpark.netlify.app/python/2020/12/11/python1.html), abstract method(https://dojang.io/mod/page/view.php?id=2389), 모듈 만들기 (https://dojang.io/mod/page/view.php?id=2447)

# 대학교 딥러닝



# 자연어처리


# 부스트캠프 ipynb

# 행렬곱

행렬곱에는 mm. matmul 뿐 아니라 sparse matrix에서 사용하는 smm, matrix multiplication만 하는 것은 아니지만 빠른 계산 속도를 위한 addmm이 있고, 복잡한 계산을 할 때 유용한 einsum도 있다. 

torch.einsum(“ij,jk->ik”, A, B)

  

# norm

torch.linalg.norm으로 브로드캐스팅을 통해 벡터와 행렬 모두 계산할 수 있지만

torch.linalg.vector_norm, torch.linalg.matrix_norm을 이용하는 것이 유지 보수 측면에서 안정적임.

  

# torch.Tensor와 torch.tensor의 차이

일단 기본적으로 대문자가 들어가는 것은 클래스, 안들어가는 것은 함수이다.

torch도 이런 규칙을 따른다.

  

torch.Tensor는 클래스로 int로 입력해도 float로 변환하고, torch가 입력되면 해당 메모리를 그대로 사용하지만, list 또는 ndarray 입력시 이를 복사하여 새로운 tensor를 만든다.

torch.tensor는 함수로 int 입력시 int 그대로 사용하고, 입력받은 데이터를 새로운 메모리 공간으로 복사 후 사용한다.

  

# index_select

```python
>>> A = torch.Tensor([[1, 2], [3, 4]])
>>> torch.index_select(A, 1, torch.tensor([0]))
tensor([[1.],
        [3.]])
>>> A[:, 0]
tensor([1., 3.])
```

슬라이싱으로 충분히 할 수 있지만
정확히는 index_select는 행렬의 형태를 보존하고 슬라이싱보다 약 40% 빠르다.

```python
>>> import timeit
>>> timeit.timeit(lambda: a[:, [1,2,3]], number=1000000)
2.824614666000002
>>> timeit.timeit(lambda: a[:, (1,2,3)], number=1000000)
2.8065428340000267
>>> timeit.timeit(lambda: torch.index_select(a, 1, torch.tensor([1,2,3])), number=1000000)
1.7120934580000267
```

# 2D gather

gather은 규칙없이 지정된 위치의 원소들을 가져올 수 있기 때문에 슬라이싱으로 할 수 없는 것을 할 수 있다.

```python
>>> A = torch.randn(3, 4)
>>> A
tensor([[ 1.6497, -0.2431,  2.0066, -0.2523],
        [-0.0983, -2.2169,  0.9605,  0.0086],
        [ 1.0067,  0.1144,  0.7044,  0.8929]])
>>> torch.gather(A, 1, torch.tensor([[0], [1]]))
tensor([[ 1.6497],
        [-2.2169]])
```
위 코드에서 gather은 1차원을 기준으로 0번째 행에서는 0번째 원소를, 1번째 행에서는 1번째 원소를 가져오라는 뜻으로 사용되고 있다.

3d gather과 임의의 크기의 3d gather은 일단 넘어간다.




