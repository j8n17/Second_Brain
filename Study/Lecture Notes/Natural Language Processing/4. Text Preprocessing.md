
토큰화 하기 전, 용도에 맞게 텍스트를 정제 및 정규화하는 작업이 필요하다.

- 정제 (Cleaning) : 갖고 있는 코퍼스로부터 노이즈 데이터를 제거.
- 정규화 (Normalization) : 표현 방법이 다른 단어들을 통합해 같은 단어로 만듦. ex) USA/US, uh-huh/uhhuh

---

- 대, 소문자 통합 - 영어에서 단어의 개수를 줄일 수 있는 정규화 방법
	- 하지만 무작정 통합하면 안됨, ex) US(미국)/us(우리), Bush(사람이름)
	- 그래서 문장 맨 앞 단어만 소문자 통합하는 식의 방법도 생각할 수 있지만, 문법을 지키지 않는 경우도 있기 때문에 그냥 모두 소문자로 바꾸는게 더 실용적인 해결책이 되기도 한다.
- 노이즈 단어 제거
	- 노이즈 데이터 : 아무 의미도 갖지 않는 글자(ex. 특수 문자), 분석 목적에 맞지 않는 불필요 단어
	- stop word, frequency, length 등을 기준으로 단어를 제거.
- 등장 빈도가 적은 단어
	- 텍스트 데이터에 너무 적게 등장해서 자연어처리에 도움이 되지 않는 단어가 존재.
- 길이가 짧은 단어
	- 영어에서는 길이가 짧은 단어를 삭제하는 것만으로도 효과가 있음.
	- 부가적인 이유로는 단어가 아닌 구두점까지 제거하기 위함.
	- 하지만 평균 길이가 2~3정도인 한국어 단어에서는 유효하지 않을 수 있음.
- Stopword(불용어)
	- 큰 의미가 없는 단어를 제거
	- I, my, me, over, 조사, 접미사 같은 단어들은 자주 등장하지만 거의 기여하는 바가 없는 경우가 있음.
	- NLTK에서는 100개 이상의 stopword를 정의하고 있음. 
	  ```python
	  from nltk.corpus import stopwords
	  stop_words = set(stopwords.words('english'))
	  result = []
	  for word in word_tokens:
		  if word not in stop_words:
			  result.append(word)
	  ```
- Regular Expression(정규 표현식)
	- 파이썬 모듈 re 
		- compile
		- search
		- match
		- split
		- findall
		- finditer
		- sub
	- NLTK의 RegexpTokenizer 
	  ```python
	  from nltk.tokenize imprt RegexpTokenizer
	  text = "~~~"
	  tokenizer1 = RegexpTokenizer("[\w]+")
	  tokenizer2 = RegexpTokenizer("\s+", gaps=True)
	  print(tokenizer1.tokenize(text)) # 토큰 리스트 출력
	  ```
- Integer Encoding
	- 각 단어를 고유한 정수에 맵핑시키는 전처리.
	- Counter 사용.
		- ```from collections import Counter```
		- 모든 토큰을 한 리스트로 만들어서 Counter에 입력하면 각 토큰의 빈도수가 Dictionary로 나옴.
		- most_common 메서드로 상위 n개의 단어 추출가능.
	- NLTK의 FreqDist
		- ```from nltk import FreqDist```
		- Counter와 동일
		- most_common 메서드로 상위 n개의 단어 추출가능.
	- 이렇게 빈도수 계산한 후 enumerate을 사용한 리스트 컴프리헨션으로 빈도수 순으로 각 단어에 인덱스 부여
- Padding
	- 병렬 연산을 위해 여러 문장의 길이를 동일하게 맞춰주는 작업
- One-Hot Encoding
	- 표현하고 싶은 단어의 인덱스에 1, 다른 인덱스에 0으로 부여하는 단어의 벡터 표현 방식

- 단어 집합에 존재하지 않는 단어 : Out-Of-Vocabulary 문제
	- 'OOV'라는 단어로 인코딩