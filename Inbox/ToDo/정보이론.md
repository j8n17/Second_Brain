---
tags: [Cross-Entropy, KL-divergence, entropy]
---
확률이 높은 정보일수록 가치가 낮은 정보

따라서 사건 $e_i$의 정보량은 다음과 같이 나타낼 수 있음.
$$h(e_i) = -\mathrm{log}_2P(e_i)$$
# 엔트로피

엔트로피는 정보를 표현하는데에 필요한 최소 평균 비트수로 확률변수 x의 불확실성을 나타낸다고 할 수 있다.

x의 확률분포에 따른 기댓값을 통해 어느정도의 비트가 있어야 해당 정보를 표현할 수 있는지 계산할 수 있고, 이 비트수가 높으면 높을수록 해당 비트가 어떤 정보를 나타내는지 알기 어려우므로 x의 불확실성도 높다고 할 수 있다.

엔트로피는 정보량($-\mathrm{log}_2P(e_i)$)의 기댓값이므로, $-P(e_i)\mathrm{log}_2P(e_i)$를 모든 i에 대해서 더한 것이 엔트로피이다. 즉, 
$$H(x) = -\sum_{i=1,k}P(e_i)\mathrm{log}_2P(e_i)$$
$$H(x) = -\int_\mathrm{R} P(x)\mathrm{log}_2P(x)$$

확률분포가 균일하면 균일할수록 엔트로피는 높다. 

직관적으로는 확률분포가 결정론적이면, 즉 분포가 균일하지 않아서 언제나 같은 사건이 발생한다면 불확실성이 낮으므로 엔트로피가 낮을 것이고, 확률분포가 균일하면 불확실성이 높으므로 엔트로피가 높다고 이해할 수 있다.

==그렇다면 왜 정보량의 기댓값이 엔트로피가 된 것일까?==

# 교차 엔트로피

두 확률분포 $P$와 $Q$사이의 교차 엔트로피
==교차 엔트로피의 의미?==
$$H(P, Q) = -\sum_x p(x)\mathrm{log}_2Q(x)$$

이는 다음과 같이 정리될 수 있다.
$$\begin{align} H(P, Q) &= -\sum_x p(x)\mathrm{log}_2Q(x)
\\&= -\sum_xP(x)\mathrm{log}_2P(x) + \sum_xP(x)\mathrm{log}_2P(x) - \sum_x p(x)\mathrm{log}_2Q(x) 
\\&= H(P) + \sum_xP(x)\mathrm{log}_2 \frac{P(x)}{Q(x)}\end{align}$$
$\sum_xP(x)\mathrm{log}_2 \frac{P(x)}{Q(x)}$는 KL Divergence라고 불리는 데, 이는 Gibb's Inequality에 의해 항상 0보다 크거나 같다.

만약 P와 Q가 동일하다면 KL Divergence가 0이 되어서 $H(P, Q) = H(P)$가 되고, 동일하지 않다면 $H(P, Q) > H(P)$이다.

즉, 머신러닝에서는 $H(P, Q)$를 최소화하는 방향으로 업데이트해서 $Q$가 $P$와 동일한 분포를 가지도록 학습하는 것이다.

# KL Divergence (Kullback-Leibler divergence)

두 확률분포의 차이를 나타내는 값이다.

$$KL(P\  \| \ Q) = \sum_xP(x)\mathrm{log}_2 \frac{P(x)}{Q(x)}$$

위에서 머신러닝에서는 크로스 엔트로피를 최소화하는 방향으로 업데이트해서 Q가 P와 동일한 분포를 가지도록 학습한다고 했다.

그런데 크로스 엔트로피는 엔트로피와 KL Divergence로 구성되어 있으므로 크로스 엔트로피를 최소화하는 것은 KL Divergence를 최소화하는 것과 같다.